import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import numpy as np
import argparse
import mlflow
import mlflow.pytorch
from mlflow.models import infer_signature
from torch.utils.data import Subset
import os
import dvc.api
import yaml
import subprocess
import sys

def parse_args():
    parser = argparse.ArgumentParser(description="PyTorch MNIST with MLflow")
    parser.add_argument("--lr", type=float, default=0.001, help="Learning rate")
    parser.add_argument("--batch_size", type=int, default=64, help="Batch size")
    parser.add_argument("--epochs", type=int, default=3, help="Number of epochs")
    parser.add_argument("--train_size", type=int, default=10000, help="Subset of training data")
    parser.add_argument("--tracking_uri", type=str, default=None)
    return parser.parse_args()

def verify_environment_clean():
    """Gitê³¼ DVCê°€ í´ë¦°í•œ ìƒíƒœì¸ì§€ ê²€ì‚¬í•©ë‹ˆë‹¤."""
    print("ğŸ” í™˜ê²½ ìƒíƒœ ê²€ì‚¬ ì¤‘...")

    # 1. Git ìƒíƒœ ì²´í¬ (ìˆ˜ì •ëœ ì½”ë“œë‚˜ ì»¤ë°‹ë˜ì§€ ì•Šì€ data.dvc í™•ì¸)
    try:
        git_status = subprocess.check_output(["git", "status", "--porcelain"]).decode("utf-8").strip()
        if git_status:
            print("\nâŒ [ERROR] Git ìƒíƒœê°€ Dirtyí•©ë‹ˆë‹¤! ë³€ê²½ì‚¬í•­ì„ ì»¤ë°‹í•˜ì„¸ìš”.")
            print(f"--- ìˆ˜ì •ëœ íŒŒì¼ ëª©ë¡ ---\n{git_status}\n")
            return False
    except Exception as e:
        print(f"âš ï¸ Git ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return False

    # 2. DVC ìƒíƒœ ì²´í¬ (ì‹¤ì œ ë°ì´í„° ì‹¤ë¬¼ì´ .dvc íŒŒì¼ì˜ í•´ì‹œì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸)
    try:
        # dvc statusê°€ ì•„ë¬´ê²ƒë„ ì¶œë ¥í•˜ì§€ ì•Šìœ¼ë©´ í´ë¦°í•œ ìƒíƒœì…ë‹ˆë‹¤.
        dvc_status = subprocess.check_output(["dvc", "status", "--quiet"])
        # dvc statusëŠ” ë³€ê²½ì‚¬í•­ì´ ìˆìœ¼ë©´ ì—ëŸ¬ ì½”ë“œ(non-zero)ë¥¼ ë°˜í™˜í•˜ê±°ë‚˜ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    except subprocess.CalledProcessError:
        print("\nâŒ [ERROR] DVC ë°ì´í„° ìƒíƒœê°€ Dirtyí•©ë‹ˆë‹¤! 'dvc commit' ë˜ëŠ” 'dvc add'ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”.")
        return False
    except Exception as e:
        print(f"âš ï¸ DVC ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return False

    print("âœ… í™˜ê²½ì´ ê¹¨ë—í•©ë‹ˆë‹¤. í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    return True

def get_dvc_hash(dvc_file_path='data.dvc'):
    """ë¡œì»¬ì˜ .dvc íŒŒì¼ì„ ì§ì ‘ ì½ì–´ MD5 í•´ì‹œê°’ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    # mlflow run ì‹¤í–‰ ì‹œ íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ê¸° ìœ„í•´ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    full_path = os.path.join(project_root, dvc_file_path)
    
    try:
        with open(full_path, 'r') as f:
            dvc_data = yaml.safe_load(f)
            # .dvc íŒŒì¼ì˜ outs ë¦¬ìŠ¤íŠ¸ì—ì„œ md5 ê°’ì„ ê°€ì ¸ì˜´
            return dvc_data['outs'][0]['md5']
    except Exception as e:
        print(f"DVC ë©”íƒ€ë°ì´í„° ì½ê¸° ì‹¤íŒ¨: {e}")
        return "unknown"

class NeuralNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(6272, 128)
        self.fc2 = nn.Linear(128, 10)
    def forward(self, x):
        # (1, 28, 28) -> (16, 28, 28)
        x = self.conv1(x)
        x = F.relu(x)
        
        # (16, 28, 28) -> (32, 28, 28)
        x = self.conv2(x)
        x = F.relu(x)
        
        # (32, 28, 28) -> (32, 14, 14)
        x = F.max_pool2d(x, 2)

        # (32, 14, 14) -> (6272, 1)
        x = torch.flatten(x, 1)

        # (6272, 1) -> (128, 1)
        x = self.fc1(x)
        x = F.relu(x)
        
        # (128, 1) -> (10, 1)
        x = self.fc2(x)
        
        output = F.log_softmax(x, dim=1)
        return output
            
def train(model, train_loader, optimizer, epoch, log_interval):
    model.train()
    total_loss = 0
    for batch_idx, (data, target) in enumerate(train_loader):
        optimizer.zero_grad()
        output = model(data)    
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        
        if batch_idx % log_interval == 0:
            # ë°°ì¹˜ë³„ Loss ê¸°ë¡
            step = epoch * len(train_loader) + batch_idx
            mlflow.log_metric("batch_loss", loss.item(), step=step)

    avg_loss = total_loss / len(train_loader)
    mlflow.log_metric("avg_train_loss", avg_loss, step=epoch)

def setup_mlflow(tracking_uri, experiment_name, run_name=None):
    mlflow.set_tracking_uri(tracking_uri)
    
    mlflow.set_experiment(experiment_name)
    
    mlflow.enable_system_metrics_logging()
    mlflow.autolog()

    try:
        active_run = mlflow.active_run() or mlflow.start_run(run_name=run_name)
        
        print(f"MLflow: logging run_id({active_run.info.run_id}) to {tracking_uri}")
        return active_run
        
    except Exception as e:
        print(f"MLflow: Failed to initialize: {e}")
        return None

def main():
    if not verify_environment_clean():
        print("ğŸ›‘ ì¬í˜„ì„±ì„ ìœ„í•´ ë”í‹° ìƒíƒœì—ì„œëŠ” ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        sys.exit(1) # ì—ëŸ¬ ì½”ë“œë¥¼ ë‚¨ê¸°ê³  ê°•ì œ ì¢…ë£Œ
        
    args = parse_args()

    dataset_version = get_dvc_hash()

    data_path = '/home/junspring/mlflow-mnist/data'
    data_path_uri = f"file://{os.path.abspath(data_path)}"
    transform = torchvision.transforms.Compose([
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize((0.1307,), (0.3081,))
    ])

    full_train_dataset = torchvision.datasets.MNIST(root=data_path, train=True, download=False, transform=transform)
    train_dataset = Subset(full_train_dataset, np.arange(args.train_size))
    test_dataset = torchvision.datasets.MNIST(root=data_path, train=False, download=False, transform=transform)
    test_dataset = Subset(test_dataset, np.arange(1000))

    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)

    train_ds = mlflow.data.from_numpy(
            features=full_train_dataset.data[:args.train_size].numpy(),
            targets=full_train_dataset.targets[:args.train_size].numpy(),
            name="mnist_train_subset",
            source=f"file://{os.path.abspath(data_path)}",
            digest=dataset_version  # MLflow ë°ì´í„°ì…‹ ë‹¤ì´ì œìŠ¤íŠ¸ë¡œ ì‚¬ìš©
        )

    myNeuralNet = NeuralNet()
    myOptimizer = torch.optim.Adam(myNeuralNet.parameters(), lr=args.lr)

    tracking_uri = args.tracking_uri or mlflow.get_tracking_uri()
    run = setup_mlflow(tracking_uri, "MLflow MNIST Test")

    if run:
        with run:
            mlflow.log_input(train_ds, context="training")
            # ëª¨ë“  ë§¤ê°œë³€ìˆ˜ ìë™ ê¸°ë¡
            mlflow.log_params(vars(args))
            mlflow.set_tag("dvc.dataset_version", dataset_version)
            
            for epoch in range(args.epochs):
                train(myNeuralNet, train_loader, myOptimizer, epoch, log_interval=40)

            # ëª¨ë¸ Signature ë° ìƒ˜í”Œ ë°ì´í„° ì„¤ì •
            input_example = next(iter(train_loader))[0][:1].numpy()
            signature = infer_signature(input_example, myNeuralNet(torch.tensor(input_example)).detach().numpy())

            # ëª¨ë¸ ì €ì¥ (MLflow ê°€ì´ë“œ ë°©ì‹)
            mlflow.pytorch.log_model(
                pytorch_model=myNeuralNet,
                name="model",
                signature=signature,
                input_example=input_example
            )
            print(f"Run ID: {run.info.run_id} ì™„ë£Œ!")

if __name__ == "__main__":
    main()